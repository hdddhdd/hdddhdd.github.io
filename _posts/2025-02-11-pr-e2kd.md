---
layout: post
title: "[논문 리뷰] Good Teachers Explain: Explanation-Enhanced Knowledge Distillation(ECCV 2024)"
subtitle: Model Compression · Faithful Distillation · Interpretability
gh-repo: m-parchami/GoodTeachersExplain
gh-badge: [star, fork, follow]
tags: [2D Object Detection, Explainable AI, Knowledge Distillation]
comments: true
mathjax: true
author: Heejung Choi
---

{: .box-success}
**논문 요약**: 본 논문에서는 Explanation-Enhanced Knowledge Distillation 방법을 제안하며, 이를 통해 학생 모델이 단순히 교사 모델의 출력을 모방하는 것이 아니라, 교사 모델이 학습한 설명의 유사성까지 유지하도록 학습할 수 있음을 보여줌. 

## 연구 배경
- Knowledge Distillation(KD)는 큰 교사 모델의 지식을 작은 학생 모델로 전달해서 경량 모델을 효과적으로 학습하는 기법이다. 
- 하지만 기존 KD 방식에서는 학생 모델이 교사 모델과 유사한 예측 결과를 보일 수는 있어도, 학습 과정에서 동일한 의사결정 논리를 따르지 않을 가능성이 크다. 
- 이런 문제는 모델의 신뢰성을 저하시키고, 모델이 정답을 맞히더라도 잘못된 이유로 결정할 가능성을 높인다. (wrong for the right reasons)
- 특히 모델의 해석 가능성이 중요한 분야에서는 단순한 성능 향상이 아니라, 모델이 정답을 도출하는 과정 자체가 의미있게 유지되는 것이 중요하다.

## 연구의 필요성
- 기존의 KD 방법은 학생 모델의 출력(Logit)만을 교사 모델과 유사하게 만들기 때문에, 학생 모델이 교사 모델과 동일한 학습 패턴을 유지한다는 보장이 없다.
- 학생 모델이 교사 모델과 다른 입력 특징을 사용할 경우에 성능이 급격하게 저하될 수 있다.
- 본 연구에서는 교사 모델의 단순한 예측값 전달이 아니라, 교사 모델의 설명 방식(ex. Grad-CAM)을 학생 모델이 직접 학습하도록 유도하는 방법을 제안한다.
- 결과적으로 옳은 이유로 옳은 결정을 할 수 있는 모델을 학습시킨다. (right for the right reasons)


## 연구 방법론
### 💡 Train
- 학생 모델은 교사 모델의 logit 분포와 설명(explanation)을 동시에 최적화하여 학습한다.
- 추가로, 실험과정에서는 교사 모델이 평가하는데 소요되는 계산 비용을 줄이기 위해 고정된 교사 모델(fixed teacher)을 사용하는 방법 사용한다.

### 💡 Evaluation
- e<sup>2</sup>KD는 Faithful KD를 달성하기 위해 다음의 세 가지 요건을 만족해야 한다.

  1) High Agreement with Teacher (교사와의 높은 일치도)
  - 일치도는 입력데이터 𝑥에 대해 교사와 학생이 동일한 예측을 얼마나 자주 내놓는 지로 정의됨.
  - Faithful KD에서는 학생 모델이 주어진 샘플에 대해 교사 모델과 동일한 방식으로 분류할 수 있도록, 즉 일치도가 높도록 보장해야 함.
  
  2) Learning the ‘Right’ Features (‘올바른’ 특징 학습)
  - 모델이 높은 정확도를 달성했더라도, 올바른 이유로 정답을 맞힌 것이 아닐 수 있음. 
  - 예를 들어, 이러한 경우에 human annotation같은 방법을 사용해서 올바른 특징을 사용하도록 유도하면 모델의 일반화 성능이 향상될 수 있음.
  - faithful distillation은 학생 모델도 교사 모델처럼 올바른 특징을 사용할 수 있도록 보장해야 함. 

  3) Maintaining Interpretability (해석 가능성 유지)
  - 교사 모델은 특정 훈련 패러다임이나 모델 아키텍쳐에 의해 explanation에서 바람직한 속성을 나타내도록 훈련될 수 있음.
  - 이러한 속성이 학생 모델로 전달되는지 평가하기 위해 두 가지 설정을 제안함.
  - 첫째, 학생 모델의 설명이 교사 모델이 학습한 속성을 얼마나 잘 반영하는지 평가. 특정 클래스에 대한 입력 특징을 얼마나 잘 localize하는지(위치를 잘 찾아내는지) 평가.
  - 둘째, 학습되지 않은, 즉 모델 아키텍쳐 내에 내제된 prior 특성도 전이할 수 있는지 평가. 
  

## 연구 결과 및 해석
### 4.1  e<sup>2</sup>KD Improves Learning from Limited Data 
![e2kd-table2](assets/img/e2kd-table1.png)
- ResNet-34 교사 모델과 ResNet-18 학생 모델로 다양한 KD 접근법의 정확도와 일치도를 세 가지 distill 데이터셋 크기에서 비교
- Explanation method: GradCAM
- 기존의 로그 기반 KD보다 e<sup>2</sup>KD가 더 높은 정확도(accuracy)와 일치도(agreement)를 보임.
- 데이터가 적을수록 e<sup>2</sup>KD의 성능향상 정도가 큼.

![e2kd-table2](assets/img/e2kd-table2.png)
- Explanation method가 B-cos모델일 때도 e<sup>2</sup>KD가 기존 KD 방법보다 우수한 성능을 보임.

### 4.2 

## 비교 분석

이 섹션에서는 비교할 수 있는 기존의 연구들과 이 연구의 결과를 비교합니다. 어떤 점에서 우월하거나 차별화되는지, 또는 한계는 무엇인지 설명합니다.

## 결론 및 향후 연구 방향

연구의 결론을 제시하고, 앞으로의 연구 방향이나 이 연구가 어떻게 발전될 수 있을지에 대한 전망을 제시합니다.

### 참고 문헌

논문이나 기타 참고 자료들을 목록화하여 나열합니다.

## Boxes

### Notification

{: .box-note}
**Note:** 특별한 주의사항이나 참고해야 할 점을 노트로 추가합니다.

### Warning

{: .box-warning}
**Warning:** 경고 또는 주의가 필요한 내용에 대해 기술합니다.

### Error

{: .box-error}
**Error:** 오류 내용 또는 잘못된 점을 지적할 때 사용합니다.

## 추가 섹션

### 코드 예제

### 코드 예제

```javascript
function exampleFunction() {
  return "This is a code example.";
}

